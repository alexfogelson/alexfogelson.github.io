<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Data Science/Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Alex Fogelson</a>
				<nav>
					<ul>
						<li><a href="mathematics.html" >Mathematics</a></li>
						<li><a href="software_engineering.html">Software Engineering</a></li>
						<li><a href="machine_learning.html" class="active">Data Science/ML</a></li>
						
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Core Skills â€“ Data Science/Machine Learning</h1>
							<h2> Industry Experience </h2>
							<p>
								I've had the privilege of helping numerous companies, from start-ups to industry titans, with data science and machine learning related projects.
							</p>
								<l style="list-style:none">
									<li> <u>PNC Financial Services</u> </li>
									<p style="margin-left:1cm">
										As the culmination of my Master's program, I paired up with industry leaders to help them recommend products using machine learning.
										I used state-of-the-art <em>natural language processing</em>, advanced models and embeddings like <em>BERT</em> and <em>Word2Vec</em>, and the intuitions I honed throughout my
										graduate coursework to advise them on how to use the data.
									</p>
									<li> <u> Qualcomm</u> </li>
										<p style="margin-left:1cm"> A pillar of my internship project at Qualcomm was using <em>recurrent neural networks</em> to detect vulnerabilities (CWEs) in assembly code. I discussed
											a range of possible options for architectures, including LSTMs and BLSTMs, and read relevant literature on the topic of machine learning in security. 
											</p>
									<li> <u>Audition Technology </u> </li>
										<p style="margin-left:1cm"> With incomplete data on noise exposure, I was challenged to approximate harm levels with just a modicum of data. This meant creating novel algorithms
											to extrapolate from the data I had, tailored to my specific situation. This wouldn't have been possible without understanding deeply the tools at my disposal, where the pitfalls were, and how to arrive where we did.
										</p>
									
								</l>
							<h2> Academic Experience </h2>
							<p>
								My academic experience with machine learning started during my junior year at Carnegie Mellon. By the end of my senior year, I had completed a Master's in Data Analytics, 
							concentrating on the latest techniques in machine learning and their application to sciences. Below are the highlights of the coursework in this field, although the full set can be found on my resume.
							</p>
							
							<l style="list-style: none">
								<li> <u>Computational Modeling, Statistical Analysis, and Machine Learning for Science</u></li>
									<p style="margin-left:1cm">
										Taught by Professor Olexandr Isayev, we learned techniques such as <em>Principle Component Analysis</em>, <em>k-means clustering</em>, <em>k-means++</em>, 
										<em>agglomerative and divisive clustering</em>, <em> DBSCAN </em> and other density-based clustering techniques, <em>Bayesian models</em>, <em>linear and logistic regressions</em>,
										 <em>generalized linear models</em>, <em>support vector machines</em>, <em>decision trees</em>, <em> ensembles</em>, and much more. 
									</p>
									
								<li> <u>Neural Networks for Science</u></li>
									<p style="margin-left:1cm">
										In another course taught by Professor Isayev, I explored the whole landscape of machine learning techniques including, starting by writing my own neural networks, <a href="https://github.com/alexfogelson/feedforward">from scratch</a>.
										I then learned about <em>feedforward networks</em>, <em>convolutional neural networks</em> and the complexities that they come with, <em>recurrent neural networks</em> including <em>LSTMs and BLSTMs</em>,
										many <em>sequence to sequence</em> models including <em>transformers</em>, and other topics such as <em>graph neural networks</em>, <em>autoencoders</em>, <em>regularization</em>, and even <em>GANs</em>.
									</p>
								<li> <u>Computer Vision</u> </li>
								<p style="margin-left:1cm">
									Computer Vision took me on a tour of a burgeoning field and the difficulties it comes with. Throughout the course, I got a taste 
									of many different techniques. We began with image processing, including filters, identifying textures, image descriptors such as <em>FAST</em> or <em>BRIEF</em>, and <em>Fourier transforms</em>.
									Next, we delved into geometry and motion, including camera models and <em>planar homographies</em> between them, as well as techniques for image alignment, 
									tracking motion and flow with algorithms such as <em>Lucas-Kanade tracking</em>. During the latter half of the semester, we began exploring <em>deep learning</em> techniques, <em>convolutional neural networks</em>, and <em>GANs</em> specifically
									for recognition and generation problems. 
								</p>
								<li> <u> Neural Computation</u></li>
								<p style="margin-left:1cm">
									As part of Carnegie Mellon's world-renowned computational biology department, Neural Computation merged deep learning concepts with the latest neuroscience research. We began with the connection between Hebbian learning and Long-Term Potentiation, then moved through topics like PCA and deep belief networks
									with the relevant biological background. As we moved into topics closer to deep learning, we discussed associative memory models, such as the Boltzmann machine 
									or Hopefield networks. Through various neuroimaging techniques, we saw similarities between the way many animals interpret visual signals to the latest research in
									convolutional neural networks. Finally, we dug into topics such as backpropagation and reinforcement learning, and their analogs in many central nervous systems.
								</p>

								<li> <u> Large Scale Computing (Supercomputers!)</u></li>
								<p style="margin-left:1cm">
									As part of my Master's program at Carnegie Mellon, I used the <em>Pittsburgh Super Computing Center's</em> new behemoth called Bridges2 to learn techniques in
									so-called Big Data. I employed techniques specific to clustering large datasets, reducing their dimensionality, and importantly became familiar with
									the widespread <em>MPI</em> framework, essential to efficiently parallelizing computing in data analysis.
								</p>
							</l>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>